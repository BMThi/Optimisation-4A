{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports sous Notebook Python\n",
    "Dans ce TP, vous allez essentiellement programmer des classes dans un fichier que vous pourrez garder pour plus tard. Ces classes seront enregistrées dans des fichiers `Optim.py`  et `functions.py`. Cependant, le comportement par défaut d'un Notebook quand on demande d'importer un fichier est de ne pas le relire !!! Ainsi vos modifications dans les fichiers ne seront pas prises en compte. Pour que ce soit le cas, il faut lancer les commandes suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctionnement des classes et des fichiers de librairie sous python\n",
    "Dans cette section, nous allons nous chauffer un peu et apprendre (si on ne le sait pas déjà) comment fonctionnent les classes et les fichiers sous python. Le fichier `Optim.py` est vide mais le fichier `functions.py` contient déjà une classe nommée `square`. Ouvrez le fichier `function.py`. Cette classe `square` a 4 sous-fonctions, la fonction `__init__` se lance à l'appel de la classe (instanciation) et les autres fonctions se lancent avec les commandes suivantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as func\n",
    "print(\"***** INSTANCIATION\")\n",
    "J=func.square()\n",
    "print(\"***** AUTRES FONCTIONS\")\n",
    "a=np.array([1,2])\n",
    "print(J.value(a))\n",
    "print(J.grad(a))\n",
    "print(J.Hess(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation de nouvelles fonctions\n",
    "\n",
    "\n",
    "> **TODO** : Dans le fichier `functions.py`, créez une classe nommée `Rosen()` sur le modèle de `square()` qui calcule la fonction, le gradient ou la Hessienne de :\n",
    "$$ f:(x,y)\\mapsto 100*(y-x^2)^2 +(1-x)^2$$\n",
    "Créez aussi une fonction `oscill()` qui calcule la fonction, le gradient ou la Hessienne de :\n",
    "$$ g:(x,y)\\mapsto \\frac 1 2 x^2 +x\\cos(y)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as func\n",
    "\n",
    "R=func.Rosen()\n",
    "O=func.oscill()\n",
    "a=np.array([1.3,2.45])\n",
    "print(R.value(a)) # 57.85\n",
    "print(O.value(a)) # -0.15630063026149965\n",
    "print(R.grad(a))\n",
    "print(R.Hess(a))\n",
    "print(O.grad(a))\n",
    "print(O.Hess(a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests de dérivées numériques\n",
    "Volontairement, je ne vous ai pas donné les réponses pour le gradient et la Hessienne. Avant de continuer, il faut vérifier que vos calculs sont bons. Pour cela on va faire des tests avec le gradient et la dérivée numérique. Pour ce faire on va partir d'un point $a$ quelconque et on prend une direction $d$ aléatoire. On compare ensuite pour plusieurs valeurs de $\\varepsilon$ entre 1.e-8 et 1.e-1 les valeurs suivantes :\n",
    "$$\\frac{J(a+\\varepsilon d) -J(a)}{\\varepsilon} \\simeq (\\nabla J(a),d)$$\n",
    "$$\\frac{\\nabla J(a+\\varepsilon d) -\\nabla J(a))}{\\varepsilon} \\simeq HJ(a)d$$\n",
    "On rappelle que quand on compare deux nombres $b$ et $c$, on s'intéresse au nombre $b/c$. Quand on compare deux vecteurs $b$ et $c$, on s'intéresse au ratio des normes et à l'angle donné par \n",
    "$$\\frac{(b,c)}{\\Vert b \\Vert \\Vert c\\Vert}$$\n",
    "Créez une fonction `deriv_num(J,a,d,compute_grad=True,compute_Hess=True)` dans `Optim.py` qui teste la dérivée numérique d'une fonction `J`. Les arguments `compute_grad` et `compute_Hess` sont optionnels et déterminent si on doit vérifier le calcul de `J` pour son gradient et sa Hessienne. Ensuite testez votre code pour les 3 fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Optim as opt\n",
    "np.random.seed(42)\n",
    "a=np.random.randn(2)\n",
    "d=np.random.randn(2)\n",
    "opt.deriv_num(func.square(),a,d)\n",
    "opt.deriv_num(func.Rosen(),a,d)\n",
    "opt.deriv_num(func.oscill(),a,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode de gradient à pas fixe\n",
    "Nous allons implémenter une méthode de gradient à pas fixe, pour cela nous allons lancer la fonction `main_algorithm` dans `Optim.py`. Cette méthode vous est donnée, il n'y a rien à modifier, il faut comprendre le code et les informations suivantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Optim as opt\n",
    "x0=np.array([7,1.5])\n",
    "f=func.square()\n",
    "res=opt.main_algorithm(f,0.1,x0,ls=opt.ls_constant,dc=opt.dc_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vous donne aussi une fonction `graphical_info` qui permet de donner des informations sur le tableau `res` de convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_and_f_plot(res,function, levels=None,xmin=-2,xmax=2,ymin=-2,ymax=2):\n",
    "    xiter=np.array(res['list_x'])\n",
    "    fig, axarr = plt.subplots(2, 2, figsize=(16,8))\n",
    "    # First plot \n",
    "    axarr[0,0].set_title('Points and levelset')\n",
    "    Nx = 1000\n",
    "    Ny = 1000\n",
    "    x = np.linspace(xmin,xmax,Nx)\n",
    "    y = np.linspace(ymin,ymax,Ny)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z=function.value((X,Y))\n",
    "    if levels:\n",
    "        CS = axarr[0,0].contour(X, Y, Z, levels)\n",
    "    else:\n",
    "        CS = axarr[0,0].contour(X, Y, Z)\n",
    "    axarr[0,0].plot(xiter[:,0], xiter[:,1],'+')\n",
    "    axarr[0,0].clabel(CS, inline=1, fontsize=10)\n",
    "    axarr[0,0].axis('equal')\n",
    "    # Second plot\n",
    "    axarr[0,1].set_title('Evolution of the cost')\n",
    "    fiter=np.array(res['list_costs'])\n",
    "    if min(fiter) > 0:\n",
    "        axarr[0,1].semilogy(fiter)\n",
    "    else:\n",
    "        axarr[0,1].plot(fiter)\n",
    "    #Third plot\n",
    "    axarr[1,0].set_title('Norm of the gradient')\n",
    "    giter=np.array(res['list_grads'])\n",
    "    axarr[1,0].semilogy(giter)\n",
    "    #Fourth plot\n",
    "    axarr[1,1].set_title('Steps')\n",
    "    siter=np.array(res['list_steps'])\n",
    "    axarr[1,1].plot(siter)\n",
    "    plt.show()\n",
    "\n",
    "f=func.square()\n",
    "res=opt.main_algorithm(f,0.1,x0,ls=opt.ls_constant,dc=opt.dc_gradient,verbose=False)\n",
    "contour_and_f_plot(res,f,xmax=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**TODO** :Essayer les 3 fonctions avec plusieurs pas et affichez le nombre d'itérations nécessaires et la valeur finale de la fonction obtenue et la valeur finale du gradient, vous devez faire apparaître le fait qu'un pas trop petit ou trop grand ne fait pas converger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez garder ici quelques tests qui vous semblent intéressant sous le modèle de `TEST 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST 1 convergent test for the function 'square': \n",
    "#f=func.square()\n",
    "#x0=np.array([-1,1])\n",
    "#res=opt.main_algorithm(f,0.1,x0,ls=opt.ls_constant,dc=opt.dc_gradient,verbose=False)\n",
    "#contour_and_f_plot(res,f,levels=[0,0.1,0.3,0.5,1,1.5,2,2.5,3,3.5,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line search Backtracking\n",
    "Implémenter une fonction de recherche linéaire `ls_backtracking` qui calcule un pas par rebroussement, c'est à dire qui vérifie que la fonction décroît et qui divise le pas par 2 si elle ne décroît pas.\n",
    "La fonction doit s'écrire sous la forme\n",
    "\n",
    "`x2,f2,df2,step2,info = ls_backtracking(x, function, step, descent,f,df)`\n",
    "\n",
    "Les arguments en entrée sont \n",
    "- `x` : l'itéré actuel\n",
    "- `function` : qui est la fonction que l'on minimise\n",
    "- `step` : qui est le pas initial de la line-search\n",
    "- `descent` : la direction de descente\n",
    "- `f` : la valeur de la fonction au point `x`\n",
    "- `df` : la valeur du gradient de la fonction au point `x`\n",
    "\n",
    "Les arguments en sortie sont \n",
    "- `x2` : le nouvel itéré, il vaut `x+step2*descent`\n",
    "- `f2` : la valeur de la fonction au point `x2`\n",
    "- `df2` : la valeur du gradient de la fonction au point `x2`\n",
    "- `step2` : le pas calculé par la méthode\n",
    "- `info` : une information, ici elle est inutile, on la met à `None`\n",
    "\n",
    "\n",
    "Tester l’algorithme de descente de gradient avec cette recherche linéaire et observer que cette recherche linéaire est plus stable que la précédente. \n",
    "Vérifier que pour `step=1` la méthode avec `ls_constant` **diverge** pour la fonction `Rosen` mais elle **converge** avec `ls_backtracking`. Montrez que le coût supplémentaire de cette méthode est négligeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=func.Rosen()\n",
    "x0=np.array([-1,-1])\n",
    "res=opt.main_algorithm(f,0.55,x0,ls=opt.ls_backtracking,dc=opt.dc_gradient,verbose=True)\n",
    "contour_and_f_plot(res,f,[0,1,10,100,1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Line search\n",
    "\n",
    "Implémenter une fonction de recherche linéaire `ls_partial_linesearch` qui calcule le pas $s_{k+1}$ parmi \n",
    "\n",
    "$$\\{0.1s_{k}, 0.5s_{k}, s_{k}, 2s_{k}, 10s_{k}\\}$$\n",
    "\n",
    "qui minimise $f(x_{k} + sd_{k})$.  \n",
    "Tester à nouveau l’algorithme de gradient et comparer la variable `nb_computations` entre cette méthode et les précédentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme de Wolfe\n",
    "On rappelle les conditions de Wolfe:\n",
    "\n",
    "$$f(x_{k} + sd_{k}) \\leq f(x_{k}) + \\epsilon_{1}s(\\nabla f(x_{k})^{T}d_{k})$$\n",
    "\n",
    "$$\\nabla f(x_{k} + sd_{k})^{T}d_{k} \\geq \\epsilon_{2}(\\nabla f(x_{k})^{T}d_{k})$$\n",
    "\n",
    "avec, en pratique: $\\epsilon_{1} = 10^{−4}$ et $\\epsilon_{2} = 0.9$.  \n",
    "Implémenter une fonction `ls_wolfe`. Cette fonction devra rendre dans la variable `info` le nombre d'itération qu'elle utilise pour converger. Tester l’algorithme de gradient avec pas de Wolfe sur les $3$ fonctions tests proposées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme de Newton\n",
    "Dans cette deuxième partie, nous allons implémenter les algorithmes de Newton. Il s'agit de prendre comme direction de descente $d_k$ solution de \n",
    "$$d_k=Hf(x_{k})^{−1}\\nabla f(x_{k})$$\n",
    "Attention ce choix de $d_k$ ne donne pas toujours une direction de descente. On va donc calculer l'angle entre $d_k$ et $\\nabla f(x_{k})$, i.e, on calcule\n",
    "\n",
    "$$\\cos(\\theta_k)=\\frac{\\langle d_k,-\\nabla f(x_k)\\rangle }{\\Vert d_k \\Vert \\Vert \\nabla f(x_k)\\Vert}$$\n",
    "\n",
    "Si $\\cos(\\theta_k)>0.1$ alors l'algorithme de Newton rend $d_k$ sinon il se transforme en algorithme de gradient et rend $-\\nabla f(x_k)$.\n",
    "Essayez la méthode de Newton avec pas constant. De préférence avec un pas de $1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tester la méthode de Newton avec pas de Wolfe sur les 3 fonctions et comparer les résultats obtenus avec ceux des algos de gradient avec pas de Wolfe et l’algorithme de Newton classique. On essaiera aussi une nouvelle fonction linesearch qui met comme premier pas de Wolfe le pas 1. Elle est définie de la manière suivante\n",
    "\n",
    "`def ls_wolfe_step_is_one(x,function,step,descent,f,df) :\n",
    "    return ls_wolfe(x,function,1.,descent,f,df)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recherche linéaire de Wolfe par interpolation cubique.\n",
    "\n",
    "On pose: $\\varphi_{k}(x) = f(x_{k} + xd_{k})$. Rappelons que: $\\varphi'_{k}(x) = \\nabla f(x_{k} + xd_{k})^{T}d_{k}$.\n",
    "\n",
    "**(a)** Soit $s > 0$ fixé. Calculer (**à la main !**) le polynôme de degré $3: p_{s}(x) = a + bx +\n",
    "cx^{2} + dx^{3}$ qui approxime $\\varphi_{k}$ sur $[0, s]$ de sorte que:\n",
    "\n",
    "$$p_{s}(0) = \\varphi_{k}(0), \\quad p_{s}(s) = \\varphi_{k}(s), \\quad p_{s}'(0) = \\varphi_{k}'(0), \\quad p_{s}'(s) = \\varphi_{k}'(s).$$\n",
    "\n",
    "**(b)** Montrer que le minimum de $p_{s}$ est atteint en un $x_{s}$ tel que: $b + 2cx_{s} + 3dx^{2}_{s} = 0$.\n",
    "\n",
    "**(c)** Implémenter une fonction *ls_wolfe_cubique* permettant de calculer un pas $s > 0$ vérifiant les conditions de Wolfe selon l’algorithme suivant:\n",
    "\n",
    "Initialisation: $s = 1$ (pas de Newton);  \n",
    "Tant que les conditions de Wolfe ne sont pas vérifiées par le pas courant $s$, faire :\n",
    "\n",
    "i-) Calculer le point $x_{s}$ qui réalise le minimum de $p_s$ sur $[0, s]$.  \n",
    "ii-) S’il existe, $s \\leftarrow x_{s}$; sinon $s \\leftarrow s/2$.\n",
    "\n",
    "Tester la méthode de Newton avec pas de Wolfe cubique sur les fonctions $f_{i}$, $i = 1, 2, 3$ et comparer les résultats obtenus avec ceux de l’algorithme de Newton avec pas de Wolfe (non cubique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
